The document titled "Evaluation of User Interfaces - Field Studies & Evaluating without Users" is a lecture presentation by Prof. Dr. Jürgen Steimle from Saarland University, Computer Science, for the Human-Computer Interaction course during the Winter Term 2024/25. The lecture focuses on field studies and analytical evaluation methods that do not involve direct user participation, such as heuristic evaluation, cognitive walkthroughs, and model-based evaluation.

### Key Topics Covered:

1. **Field Studies**:
   - **Definition**: Research conducted in natural settings (e.g., workplaces, homes) to understand how users interact with technology in real-world contexts.
   - **Purpose**: To identify opportunities for new technology, determine design requirements, and evaluate technology in use.
   - **Methods**: Observation, interviews, diaries, and log data analysis.

2. **Observation**:
   - **Direct Observation**: The evaluator watches users interact with the system, either silently or with the user thinking aloud.
   - **Constructive Interaction**: Two users work together on a task, and their conversation is observed to gain insights into their thought processes.
   - **Data Recording**: Methods include taking notes, audio recordings, video recordings, and photographs.

3. **Log Data Analysis**:
   - **Definition**: Indirect observation where the system logs user activities automatically.
   - **Examples**: Web page analytics, wearable device logging.
   - **Benefits**: Provides detailed quantitative data without disrupting the user.
   - **Challenges**: Limited to what the system logs, and privacy concerns may arise.

4. **Diaries**:
   - **Definition**: Users provide regular feedback over time, often in the form of short questionnaires or descriptions of specific events.
   - **Purpose**: To gather longitudinal data on user behavior and experiences.

5. **Interviews**:
   - **Structured Interviews**: Tightly scripted, with high replicability but limited richness.
   - **Semi-Structured Interviews**: Guided by a script but allow for deeper exploration of interesting issues.
   - **Unstructured Interviews**: Open-ended, with no strict script, providing rich but less replicable data.
   - **Focus Groups**: Group interviews where participants discuss their opinions, often leading to consensus or highlighting conflicts.

6. **Questionnaires**:
   - **Closed Questions**: Predetermined answer formats (e.g., yes/no, multiple-choice).
   - **Open Questions**: No predefined answers, allowing for richer qualitative data.
   - **Scales**: Likert scales and semantic differential scales are commonly used to measure attitudes and opinions.

7. **Analytical Evaluation**:
   - **Heuristic Evaluation**: Experts evaluate the system against a set of usability heuristics (e.g., Nielsen’s 10 usability principles).
   - **Cognitive Walkthrough**: Experts simulate user tasks to identify potential usability issues, focusing on ease of learning.
   - **Model-Based Evaluation**: Using models like GOMS (Goals, Operators, Methods, Selection rules) to predict user performance and system efficiency.

8. **GOMS (Goals, Operators, Methods, Selection Rules)**:
   - **Definition**: A model used to predict how long it takes for users to complete tasks based on their goals and the steps required to achieve them.
   - **Keystroke Level Model (KLM)**: A refinement of GOMS that assigns specific time values to user actions (e.g., pressing a key, pointing with a mouse).

### Key Concepts and Techniques:

- **Observation Methods**: Silent observation, think-aloud, and constructive interaction are used to gather insights into user behavior and thought processes.
- **Data Recording**: Notes, audio, video, and photographs are used to capture user interactions, each with its own benefits and challenges.
- **Log Data Analysis**: Automated logging of user activities provides quantitative data but may lack context.
- **Diaries**: Longitudinal data collection through user self-reporting, useful for understanding long-term usage patterns.
- **Interviews and Questionnaires**: Tools for gathering subjective feedback from users, with structured, semi-structured, and unstructured formats.
- **Heuristic Evaluation**: A quick and cost-effective method for identifying usability problems by comparing the system against established usability principles.
- **Cognitive Walkthrough**: A method for evaluating the ease of learning a system by simulating user tasks.
- **GOMS and KLM**: Models for predicting user performance based on the time required for specific actions.

### Examples and Case Studies:

- **PLink Field Study**: A study of a paper-based linking system used by researchers in their natural work environment, with data collected through interviews, photos, and log analysis.
- **Heuristic Evaluation Example**: Using Nielsen’s 10 usability heuristics to evaluate a system, with experts identifying and prioritizing usability problems.
- **GOMS Example**: Predicting the time it takes for a user to delete a word using either the delete key or a menu option, based on the Keystroke Level Model.

### Challenges and Considerations:

- **Ethics**: Ensuring participant comfort, privacy, and informed consent is crucial in all evaluation methods.
- **Bias**: Interviewers and observers must avoid influencing participants or misinterpreting data.
- **Generalizability**: Results from controlled environments may not always apply to real-world settings.
- **Data Overload**: Field studies and log data analysis can produce large amounts of data, requiring careful analysis and interpretation.

### Conclusion:

Field studies and analytical evaluation methods provide valuable insights into how users interact with technology in real-world settings. By combining observation, interviews, questionnaires, and model-based evaluations, designers can identify usability issues, understand user behavior, and improve system design. Ethical considerations and pilot testing are essential to ensure the validity and reliability of evaluation results.
